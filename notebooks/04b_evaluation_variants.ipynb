{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04b - Evaluation of variants\n",
        "Loop over checkpoints produced by 03b/03c, save confusion matrices and reports."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "from subprocess import run\n",
        "import json\n",
        "\n",
        "# --- ROOT ---\n",
        "ROOT = Path.cwd().resolve()\n",
        "for _ in range(10):\n",
        "    if (ROOT / \"Data\").exists():\n",
        "        break\n",
        "    ROOT = ROOT.parent\n",
        "else:\n",
        "    raise FileNotFoundError(\"Project root not found (Data folder missing).\")\n",
        "\n",
        "# --- Python (prefer venv) ---\n",
        "VENV_PY = ROOT / \".venv\" / \"Scripts\" / \"python.exe\"\n",
        "PY = str(VENV_PY if VENV_PY.exists() else Path(sys.executable))\n",
        "\n",
        "# --- Paths ---\n",
        "TEST_DIR = ROOT / \"Data\" / \"raw\" / \"test\"\n",
        "CURVES = ROOT / \"Visualisation\" / \"training_curves.png\"\n",
        "\n",
        "WEIGHTS_DIR = ROOT / \"Model\" / \"weights\"\n",
        "VIS_DIR = ROOT / \"Visualisation\"\n",
        "MON_DIR = ROOT / \"Monitoring\" / \"output\"\n",
        "\n",
        "VIS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MON_DIR.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if not TEST_DIR.exists():\n",
        "    raise FileNotFoundError(f\"TEST_DIR missing: {TEST_DIR}\")\n",
        "\n",
        "weight_paths = sorted(WEIGHTS_DIR.glob(\"best_*.pt\"))\n",
        "print(\"Checkpoints found:\", len(weight_paths))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def evaluate(weights_path: Path) -> dict:\n",
        "    suffix = weights_path.stem.replace(\"best_\", \"\")\n",
        "    confusion_path = VIS_DIR / f\"confusion_{suffix}.png\"\n",
        "    report_path = MON_DIR / f\"metrics_{suffix}.json\"\n",
        "    metrics_global = MON_DIR / \"metrics.json\"\n",
        "\n",
        "    cmd = [\n",
        "        PY, \"-m\", \"Model.training.evaluate\",\n",
        "        \"--data-dir\", str(TEST_DIR),\n",
        "        \"--weights\", str(weights_path),\n",
        "        \"--confusion-path\", str(confusion_path),\n",
        "        \"--report-path\", str(report_path),\n",
        "        \"--metrics-json\", str(metrics_global),\n",
        "        \"--training-curves\", str(CURVES),\n",
        "    ]\n",
        "\n",
        "    print(\"===\", \" \".join(cmd))\n",
        "    run(cmd, check=True, cwd=str(ROOT))\n",
        "\n",
        "    return {\n",
        "        \"weights\": str(weights_path),\n",
        "        \"confusion\": str(confusion_path),\n",
        "        \"report\": str(report_path),\n",
        "        \"metrics_global\": str(metrics_global),\n",
        "    }\n",
        "\n",
        "artifacts = [evaluate(p) for p in weight_paths]\n",
        "print(json.dumps(artifacts, indent=2, ensure_ascii=False))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}