{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports et chemins\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path('../..').resolve()))\n",
    "import random\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, classification_report\n",
    "from PIL import Image\n",
    "from model.image.dataset import SimpleImageFolder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ParamÃ¨tres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ParamÃ¨tres\n",
    "classes = ['Chao', 'Milho', 'Ervas']\n",
    "image_size = 128  # resize\n",
    "per_class_limit = 800  \n",
    "batch_size = 32\n",
    "noise_std = 0.0  # 0 pour clean, 0.02-0.05 pour bruit blanc\n",
    "model_name = 'resnet18'  # resnet18/resnet34/vgg16/vgg19/mobilenet_v2\n",
    "optimizer_name = 'adam'  # adam/rmsprop/adagrad\n",
    "lr = 1e-3\n",
    "dropout = 0.3\n",
    "use_pretrained = True\n",
    "epochs = 2\n",
    "base_dir = Path('../data/ImagensTCCRotuladas').resolve()\n",
    "split_dirs = {\n",
    "    'train': base_dir / 'Treino',\n",
    "    'val': next((p for p in base_dir.iterdir() if p.name.lower().startswith('valid')), None),\n",
    "    'test': base_dir / 'Teste',\n",
    "}\n",
    "print('Classes:', classes)\n",
    "print('per_class_limit:', per_class_limit, 'image_size:', image_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explorations des splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration des splits\n",
    "from typing import Dict, List\n",
    "\n",
    "def collect_files(split: str, classes):\n",
    "    root = split_dirs[split]\n",
    "    per_class = {}\n",
    "    for cls in classes:\n",
    "        per_class[cls] = sorted([p for p in (root / cls).glob(\"*\") if p.suffix.lower() in {\".jpg\", \".jpeg\", \".png\"}])\n",
    "    return per_class\n",
    "\n",
    "train_files = collect_files(\"train\", classes)\n",
    "val_files = collect_files(\"val\", classes) if split_dirs[\"val\"] else {}\n",
    "test_files = collect_files(\"test\", classes)\n",
    "\n",
    "train_counts = {cls: len(paths) for cls, paths in train_files.items()}\n",
    "val_counts = {cls: len(paths) for cls, paths in val_files.items()} if val_files else {}\n",
    "test_counts = {cls: len(paths) for cls, paths in test_files.items()}\n",
    "print(train_counts)\n",
    "print(val_counts)\n",
    "print(test_counts)\n",
    "min_train = min(train_counts.values()) if train_counts else 0\n",
    "print(\"Min train class size:\", min_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chartBar par split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def plot_counts(counts: dict, title: str, color: str):\n",
    "    labels = list(counts.keys())\n",
    "    values = [counts[k] for k in labels]\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.bar(labels, values, color=color)\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=25)\n",
    "    for i, v in enumerate(values):\n",
    "        plt.text(i, v, str(v), ha='center', va='bottom', fontsize=8)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "plot_counts(train_counts, 'Train', 'tab:blue')\n",
    "if val_counts:\n",
    "    plot_counts(val_counts, 'Val', 'tab:orange')\n",
    "if test_counts:\n",
    "    plot_counts(test_counts, 'Test', 'tab:green')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã©chantillons visuels\n",
    "n_per_class = 3\n",
    "fig, axes = plt.subplots(len(classes), n_per_class, figsize=(3 * n_per_class, 3 * len(classes)))\n",
    "for row, cls in enumerate(classes):\n",
    "    files = train_files.get(cls, [])[:n_per_class]\n",
    "    for col, path in enumerate(files):\n",
    "        ax = axes[row, col] if len(classes) > 1 else axes[col]\n",
    "        ax.imshow(Image.open(path))\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(cls)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaders (balance + bruit blanc optionnel)\n",
    "class AddWhiteNoise:\n",
    "    def __init__(self, std: float = noise_std):\n",
    "        self.std = std\n",
    "    def __call__(self, tensor):\n",
    "        noise = torch.randn_like(tensor) * self.std\n",
    "        return torch.clamp(tensor + noise, 0.0, 1.0)\n",
    "\n",
    "def build_loader(split: str, balance: bool):\n",
    "    split_path = split_dirs[split]\n",
    "    rng = random.Random(42)\n",
    "    paths, labels = [], []\n",
    "    for cls in classes:\n",
    "        files = sorted([p for p in (split_path / cls).glob(\"*\") if p.suffix.lower() in {'.jpg', '.jpeg', '.png'}])\n",
    "        if per_class_limit:\n",
    "            files = files[:per_class_limit]\n",
    "        if balance:\n",
    "            target = per_class_limit or min_train\n",
    "            rng.shuffle(files)\n",
    "            files = files[:target]\n",
    "        for p in files:\n",
    "            paths.append(p)\n",
    "            labels.append(classes.index(cls))\n",
    "    tfms = [transforms.Resize((image_size, image_size)), transforms.ToTensor()]\n",
    "    if split == 'train' and noise_std:\n",
    "        tfms.append(AddWhiteNoise(noise_std))\n",
    "    transform = transforms.Compose(tfms)\n",
    "    ds = SimpleImageFolder(paths, labels, image_size=image_size)\n",
    "    ds.transform = transform\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=(split=='train'))\n",
    "\n",
    "train_loader = build_loader('train', balance=True)\n",
    "val_loader = build_loader('val', balance=False)\n",
    "print(\"Train size\", len(train_loader.dataset))\n",
    "print(\"Val size\", len(val_loader.dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backbones et optimisateurs\n",
    "available_models = ['resnet18', 'resnet34', 'vgg16', 'vgg19', 'mobilenet_v2']\n",
    "assert model_name in available_models\n",
    "\n",
    "def create_backbone(name: str, num_classes: int, dropout: float, use_pretrained: bool):\n",
    "    if name == 'resnet18':\n",
    "        m = models.resnet18(weights=models.ResNet18_Weights.DEFAULT if use_pretrained else None)\n",
    "        in_f = m.fc.in_features\n",
    "        m.fc = nn.Sequential(nn.Dropout(dropout), nn.Linear(in_f, num_classes))\n",
    "        return m\n",
    "    if name == 'resnet34':\n",
    "        m = models.resnet34(weights=models.ResNet34_Weights.DEFAULT if use_pretrained else None)\n",
    "        in_f = m.fc.in_features\n",
    "        m.fc = nn.Sequential(nn.Dropout(dropout), nn.Linear(in_f, num_classes))\n",
    "        return m\n",
    "    if name == 'vgg16':\n",
    "        m = models.vgg16(weights=models.VGG16_Weights.DEFAULT if use_pretrained else None)\n",
    "        in_f = m.classifier[-1].in_features\n",
    "        m.classifier[-1] = nn.Linear(in_f, num_classes)\n",
    "        return m\n",
    "    if name == 'vgg19':\n",
    "        m = models.vgg19(weights=models.VGG19_Weights.DEFAULT if use_pretrained else None)\n",
    "        in_f = m.classifier[-1].in_features\n",
    "        m.classifier[-1] = nn.Linear(in_f, num_classes)\n",
    "        return m\n",
    "    if name == 'mobilenet_v2':\n",
    "        m = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT if use_pretrained else None)\n",
    "        in_f = m.classifier[-1].in_features\n",
    "        m.classifier[-1] = nn.Sequential(nn.Dropout(dropout), nn.Linear(in_f, num_classes))\n",
    "        return m\n",
    "    raise ValueError('backbone not handled')\n",
    "\n",
    "def make_optimizer(name: str, params, lr: float):\n",
    "    name = name.lower()\n",
    "    if name == 'adam':\n",
    "        return torch.optim.Adam(params, lr=lr)\n",
    "    if name == 'rmsprop':\n",
    "        return torch.optim.RMSprop(params, lr=lr)\n",
    "    if name == 'adagrad':\n",
    "        return torch.optim.Adagrad(params, lr=lr)\n",
    "    raise ValueError('optimizer not handled')\n",
    "\n",
    "device = globals().get('device', torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "model = create_backbone(model_name, num_classes=len(classes), dropout=dropout, use_pretrained=use_pretrained).to(device)\n",
    "opt = make_optimizer(optimizer_name, model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra?nement\n",
    "from tqdm.auto import tqdm\n",
    "history = {'train_loss': [], 'val_acc': []}\n",
    "\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            preds.extend(pred.cpu().tolist())\n",
    "            labels.extend(y.cpu().tolist())\n",
    "    return labels, preds\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for x, y in tqdm(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        running += loss.item()\n",
    "    labels, preds = evaluate(val_loader)\n",
    "    val_acc = accuracy_score(labels, preds)\n",
    "    history['train_loss'].append(running / max(1, len(train_loader)))\n",
    "    history['val_acc'].append(val_acc)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - loss={history['train_loss'][-1]:.4f} val_acc={val_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbes + confusion\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axes[0].plot(history['train_loss'], label='train loss')\n",
    "axes[0].set_title('Loss')\n",
    "axes[1].plot(history['val_acc'], label='val acc')\n",
    "axes[1].set_title('Val acc')\n",
    "for ax in axes: ax.legend(); ax.grid(True)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "labels, preds = evaluate(val_loader)\n",
    "cm = confusion_matrix(labels, preds)\n",
    "ConfusionMatrixDisplay(cm, display_labels=classes).plot(xticks_rotation=45)\n",
    "plt.title('Val confusion')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rapport val + test\n",
    "val_labels, val_preds = evaluate(val_loader)\n",
    "print('VAL:', classification_report(val_labels, val_preds, target_names=classes, digits=3))\n",
    "\n",
    "test_loader = build_loader('test', balance=False)\n",
    "test_labels, test_preds = evaluate(test_loader)\n",
    "print('TEST:', classification_report(test_labels, test_preds, target_names=classes, digits=3))\n",
    "cm_test = confusion_matrix(test_labels, test_preds)\n",
    "ConfusionMatrixDisplay(cm_test, display_labels=classes).plot(xticks_rotation=45)\n",
    "plt.title('Test confusion')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commentaire: sauvegarde des artefacts dans model/registry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des artefacts dans model/registry\n",
    "from datetime import datetime\n",
    "import json\n",
    "run_id = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
    "run_dir = Path('../model/registry') / f'image_{run_id}_notebook'\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(model.state_dict(), run_dir / 'model.pt')\n",
    "summary = {\n",
    "    'classes': classes,\n",
    "    'model_name': model_name,\n",
    "    'optimizer': optimizer_name,\n",
    "    'lr': lr,\n",
    "    'dropout': dropout,\n",
    "    'use_pretrained': use_pretrained,\n",
    "    'image_size': image_size,\n",
    "    'per_class_limit': per_class_limit,\n",
    "    'noise_std': noise_std,\n",
    "    'epochs': epochs,\n",
    "    'val_samples': len(val_labels),\n",
    "    'test_samples': len(test_labels),\n",
    "}\n",
    "(run_dir / 'summary.json').write_text(json.dumps(summary, indent=2), encoding='utf-8')\n",
    "(run_dir / 'val_report.txt').write_text(classification_report(val_labels, val_preds, target_names=classes, digits=3), encoding='utf-8')\n",
    "(run_dir / 'test_report.txt').write_text(classification_report(test_labels, test_preds, target_names=classes, digits=3), encoding='utf-8')\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axes[0].plot(history['train_loss']); axes[0].set_title('Loss')\n",
    "axes[1].plot(history['val_acc']); axes[1].set_title('Val acc')\n",
    "for ax in axes: ax.grid(True)\n",
    "fig.savefig(run_dir / 'curves.png', dpi=160, bbox_inches='tight'); plt.close(fig)\n",
    "ConfusionMatrixDisplay(cm, display_labels=classes).plot(xticks_rotation=45)\n",
    "plt.title('Val confusion'); plt.savefig(run_dir / 'val_confusion.png', dpi=160, bbox_inches='tight'); plt.close()\n",
    "ConfusionMatrixDisplay(cm_test, display_labels=classes).plot(xticks_rotation=45)\n",
    "plt.title('Test confusion'); plt.savefig(run_dir / 'test_confusion.png', dpi=160, bbox_inches='tight'); plt.close()\n",
    "print('Artifacts saved to', run_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ProbabilitÃ©s sur quelques images test\n",
    "sample_images = list((split_dirs['test'] / classes[0]).glob('*.jpg'))[:3]\n",
    "softmax = nn.Softmax(dim=1)\n",
    "model.eval()\n",
    "for img_path in sample_images:\n",
    "    tensor = transforms.Compose([transforms.Resize((image_size, image_size)), transforms.ToTensor()])(Image.open(img_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        probs = softmax(model(tensor)).squeeze(0)\n",
    "    top_probs, top_idx = torch.topk(probs, k=len(classes))\n",
    "    print(img_path.name, [(classes[i], float(p)) for p, i in zip(top_probs, top_idx)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commentaire: LIME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIME superpixels explicatifs\n",
    "try:\n",
    "    from lime import lime_image\n",
    "    import numpy as np\n",
    "    from skimage.segmentation import mark_boundaries\n",
    "except ImportError:\n",
    "    print('lime ou scikit-image non install?s; saute cette cellule')\n",
    "else:\n",
    "    explainer = lime_image.LimeImageExplainer()\n",
    "    img_path = sample_images[0] if sample_images else None\n",
    "    if img_path is None:\n",
    "        print('Pas d\\'image test dispo')\n",
    "    else:\n",
    "        img = Image.open(img_path).convert('RGB').resize((image_size, image_size))\n",
    "        img_np = np.array(img)\n",
    "        def predict_fn(xs):\n",
    "            arr = torch.tensor(xs).permute(0,3,1,2).float() / 255.0\n",
    "            arr = arr.to(device)\n",
    "            with torch.no_grad():\n",
    "                logits = model(arr)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "            return probs.cpu().numpy()\n",
    "        exp = explainer.explain_instance(img_np, predict_fn, top_labels=1, hide_color=0, num_samples=200)\n",
    "        temp, mask = exp.get_image_and_mask(exp.top_labels[0], positive_only=True, num_features=5, hide_rest=False)\n",
    "        plt.figure(figsize=(4,4))\n",
    "        plt.imshow(mark_boundaries(temp, mask))\n",
    "        plt.title(f'LIME: {img_path.name}')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
